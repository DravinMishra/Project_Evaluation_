# -*- coding: utf-8 -*-
"""Insurance_Claim_Fraud_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ynWCizCG13eJYIqG68OotLAXdHXYs4Hf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

ins_data = pd.read_csv('https://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Insurance%20Claim%20Fraud%20Detection/Automobile_insurance_fraud.csv')
ins_data

ins_data.shape

"""*There are 999 rows and 39 columns in our dataset.*"""

ins_data.info()

columns = ins_data.columns
print(columns)

# Create a dictionary mapping old column names to new descriptive names
column_mapping = {
    '328': 'months_as_customer',
    '48': 'age',
    '521585': 'policy_number',
    '17-10-2014': 'policy_bind_date',
    'OH': 'policy_state',
    '250/500': 'policy_csl',
    '1000': 'policy_deductible',
    '1406.91': 'policy_annual_premium',
    '0': 'umbrella_limit',
    '466132': 'insured_zip',
    'MALE': 'insured_sex',
    'MD': 'insured_education_level',
    'craft-repair': 'insured_occupation',
    'sleeping': 'insured_hobbies',
    'husband': 'insured_relationship',
    '53300': 'capital-gains',
    '0.1': 'capital-loss',
    '25-01-2015': 'incident_date',
    'Single Vehicle Collision': 'incident_type',
    'Side Collision': 'collision_type',
    'Major Damage': 'incident_severity',
    'Police': 'authorities_contacted',
    'SC': 'incident_state',
    'Columbus': 'incident_city',
    '9935 4th Drive': 'incident_location',
    '5': 'incident_hour_of_the_day',
    '1': 'number_of_vehicles_involved',
    'YES': 'property_damage',
    '1.1': 'bodily_injuries',
    '2': 'witnesses',
    'YES.1': 'police_report_available',
    '71610': 'total_claim_amount',
    '6510': 'injury_claim',
    '13020': 'property_claim',
    '52080': 'vehicle_claim',
    'Saab': 'auto_make',
    '92x': 'auto_model',
    '2004': 'auto_year',
    'Y': 'fraud_reported'
}

# Step 1: Add the current column names as the first row of the DataFrame
ins_data.loc[-1] = ins_data.columns
ins_data.index = ins_data.index + 1  # Shift the index
ins_data = ins_data.sort_index()  # Sort the DataFrame to put the new row at the top

# Step 2: Rename the columns using the column_mapping dictionary
ins_data.rename(columns=column_mapping, inplace=True)

# Display the updated DataFrame
print(ins_data.head())

# ins_data.loc[-1] = ins_data.columns
# ins_data.index = ins_data.index + 1  # shifting index
# ins_data = ins_data.sort_index()

# # Step 2: Replace the column names with new names
# new_column_names = [f'Column_{i}' for i in range(1, len(ins_data.columns) + 1)]
# ins_data.columns = new_column_names

# print(ins_data)

ins_data.columns

ins_data.info()

ins_data.columns.to_list()

ins_data.isnull().sum()

ins_data.fillna(ins_data['authorities_contacted'].mode()[0] , inplace = True)

ins_data.isnull().sum()

"""*Here , We can see that there is no null values present in our Dataset.*

# **DATA PRE-PROCESSING**
"""

# Dropping those columns , which is not in use.

ins_data.drop(columns = ['policy_number' , 'incident_location'] , axis = 1 , inplace = True)

ins_data.shape

for i in ins_data.columns :
  print(ins_data[i].value_counts())
  print("\n")

"""From the values we can observe,
*
1. The features collision_type, police_report_available, property_damage have ? as values.
2. The features umbrella_limit has a about 80% of it's values as 0, so we can drop it.
3. The feature insured_zip, is a list of unique numbers, so let us drop it.*
"""

ins_data.drop(['umbrella_limit' , 'insured_zip'] , axis = 1 , inplace = True)

"""*Let us retrieve the day, month and year from the features policy_bind_day and incident_date*"""

ins_data['policy_bind_date']=pd.to_datetime(ins_data['policy_bind_date'])
ins_data['incident_date']=pd.to_datetime(ins_data['incident_date'])

# Extracting Day, Month and Year column from policy_bind_date
ins_data['policy_bind_day'] = ins_data['policy_bind_date'].dt.day
ins_data['policy_bind_month'] = ins_data['policy_bind_date'].dt.month
ins_data['policy_bind_year'] = ins_data['policy_bind_date'].dt.year

# Extracting Day, Month and Year column from incident_date
ins_data['incident_day'] = ins_data['incident_date'].dt.day
ins_data['incident_month'] = ins_data['incident_date'].dt.month
ins_data['incident_year'] = ins_data['incident_date'].dt.year

ins_data.drop(['policy_bind_date','incident_date'],axis=1,inplace=True)

"""#### To find where the ? data is present throughout the dataset"""

ins_data[ins_data.columns[(ins_data=="?").any()]].nunique()

"""*All these columns are categorical columns, so let us fill theose values with it's mode.*"""

ins_data['collision_type'].value_counts()

ins_data['collision_type'] = ins_data.collision_type.str.replace('?' , ins_data['collision_type'].mode()[0])

ins_data['property_damage'].value_counts()

"""We will fill the "?" in the feature property_damage with the value "NO"
"""

ins_data['property_damage'] = ins_data.property_damage.str.replace('?' , 'NO')

ins_data['police_report_available'].value_counts()

"""*Let us fill the unknown value with NO*"""

ins_data['police_report_available'] = ins_data.police_report_available.str.replace('?' , "NO")

"""*The features policy_csl showing object data type but it contains numerical data,let us extratc csl_per_person and csl_per_accident from policy_csl column and then will convert their object data type into integer data type.*"""

# Extracting csl_per_person and cls_per_accident from policy_csl column.

ins_data['csl_per_person'] = ins_data.policy_csl.str.split('/' , expand=True)[0]
ins_data['csl_per_accident'] = ins_data.policy_csl.str.split('/' , expand = True)[1]

# Converting object datatype into integer data type
ins_data['csl_per_person']=ins_data['csl_per_person'].astype('int64')
ins_data['csl_per_accident']=ins_data['csl_per_accident'].astype('int64')

# since we ectracted the data from policy_csl column , So let's rop it.

ins_data.drop('policy_csl' , axis = 1 , inplace = True)

ins_data['auto_year'] = ins_data['auto_year'].astype('int64')

# Lets extract age of the vehicle from auto_year by subtracting it from the year 2018
ins_data['Vehicle_Age']=2021-ins_data['auto_year']
ins_data.drop("auto_year",axis=1,inplace=True)

# checking for unique value present in dataset

ins_data.nunique().to_frame("Number of unique values")

"""*The column incident_year has only 1 value ,  So we can drop that column.*"""

# Dropping incident_year column

ins_data.drop('incident_year' , axis = 1 , inplace = True)

ins_data.head()

ins_data.shape

ins_data.info()

"""*Seperating the categorical and numerical column.*"""

ins_data['months_as_customer'] = ins_data['months_as_customer'].astype('int64')

ins_data['age'] = ins_data['age'].astype("int64")
ins_data['policy_deductible'] = ins_data['policy_deductible'].astype("int64")
ins_data['capital-gains'] = ins_data['capital-gains'].astype('int64')
ins_data['number_of_vehicles_involved'] = ins_data['number_of_vehicles_involved'].astype('int64')
ins_data['incident_hour_of_the_day'] = ins_data['incident_hour_of_the_day'].astype("int64")
ins_data['witnesses'] = ins_data['witnesses'].astype("int64")
ins_data['injury_claim'] = ins_data['injury_claim'].astype('int64')
ins_data['property_claim'] = ins_data['property_claim'].astype('int64')
ins_data['vehicle_claim'] = ins_data['vehicle_claim'].astype('int64')
ins_data['policy_bind_day'] = ins_data['policy_bind_day'].astype('int64')
ins_data['policy_bind_month'] = ins_data['policy_bind_month'].astype('int64')
ins_data['policy_bind_year'] = ins_data['policy_bind_year'].astype('int64')
ins_data['incident_month'] = ins_data['incident_month'].astype('int64')
ins_data['Vehicle_Age'] = ins_data['Vehicle_Age'].astype('int64')

# Convert the 'bodily_injuries' column to integers, ignoring any non-integer values

ins_data['total_claim_amount'] = ins_data['total_claim_amount'].astype('int64')
ins_data['bodily_injuries'] = ins_data['bodily_injuries'].astype('int64', errors='ignore') # Convert to int, ignoring errors
ins_data['capital-loss'] = ins_data['capital-loss'].astype("int64" , errors='ignore')

ins_data.info('policy_annual_premium')

ins_data['policy_annual_premium'] = ins_data['policy_annual_premium'].astype("int64" , errors = 'ignore')

# Checking for categorical columns :
cat_col = []
for col in ins_data.dtypes.index :
  if ins_data.dtypes[col] == 'object' :
    cat_col.append(col)
print("Categorical columns are : \n" , cat_col)
print("\n")

# Checking for numerical columns
numerical_col = []
for i in ins_data.dtypes.index:
    if ins_data.dtypes[i]!='object':
        numerical_col.append(i)
print("Numerical columns are:\n",numerical_col)
print("\n")

# checking for unique values in target columns :
ins_data['fraud_reported'].unique()

# Checking the list counts of target

ins_data['fraud_reported'].value_counts()

"""*As we can see that our dataset is imbalanced , so we need to balance it. We will use oversampling and balance the data later.*"""

ins_data.describe()

"""# **EDA : EXPLORATORY DATA ANALYSIS**

# ***Uni-Variate Analysis***
"""

# Visualizing how many insurance claims is fraudulent.

print(ins_data['fraud_reported'].value_counts())
sns.countplot(x = ins_data['fraud_reported'] , palette = 'Set3')
plt.show()

# Visualizing how many policy states are present in the dataset.

print(ins_data['policy_state'].value_counts())
sns.countplot(x = ins_data['policy_state'] , palette = 'PuRd')
plt.show()

# Visualizing count of customers insured education level are present in the dataset
print(ins_data['insured_education_level'].value_counts())
sns.countplot(x = ins_data['insured_education_level'],palette='PuRd')
plt.show()

# Visualizing count of customers insured occupation are present in the dataset
print(ins_data['insured_occupation'].value_counts())
sns.countplot(x = ins_data['insured_occupation'],palette='RdBu')
plt.xticks(rotation=90)
plt.show()

# Visualizing count of insured sex are present in the dataset
print(ins_data['insured_hobbies'].value_counts())
sns.countplot(x = ins_data['insured_hobbies'],palette='PuBu')
plt.xticks(rotation =90)
plt.show()

# Visualizing count of what kind of relationship the isured cutomers are in the dataset
print(ins_data['insured_relationship'].value_counts())
sns.countplot(x = ins_data['insured_relationship'],palette='PRGn')
plt.xticks(rotation = 45)
plt.show()

# Visualizing count of insured sex are present in the dataset
print(ins_data['incident_type'].value_counts())
sns.countplot(x = ins_data['incident_type'],palette='Spectral')
plt.xticks(rotation = 45)
plt.show()

# Visualizing count of insured sex are present in the dataset
print(ins_data['collision_type'].value_counts())
sns.countplot(x = ins_data['collision_type'],palette='OrRd')
plt.show()

"""# *Checking for the distribution of Skewness.*"""

plt.figure(figsize = (25 , 35) , facecolor = 'white')
plotnumber = 1
for col in numerical_col :
  if plotnumber<=23 :
    ax = plt.subplot(8,3,plotnumber)
    sns.distplot(ins_data[col] , hist=False)
    plt.xlabel(col , fontsize = 20)
  plotnumber+=1
plt.show()

"""From the above distribution plots:
1. Most of the features are normally distributed.
2. The feature capital_gains is right skewed.
3. The feature capital_loss is left skewed.

# ***Bi-Variate Analysis***
"""

# Comparing poicy_state and fraud_reported

plt.figure(figsize = (10 , 8))
sns.countplot(x = ins_data['policy_state'] , data = ins_data , hue = 'fraud_reported')
plt.title("POLICY_STATES vs FRAUD_REPORTED")
plt.show()

"""*Here we can see that Fraud report is the highest in "OH".*"""

# Comparing insured_education_level and fraud_reported.

plt.figure(figsize = (10 , 8))
sns.countplot(x = ins_data['insured_education_level'] , data = ins_data , hue = ins_data['fraud_reported'])
plt.title("EDUCATION LEVEL vs FRAUD REPORTED")
plt.show()

"""* The fraud is commited mostly by the people who have completed their JD/MD level of education.
The fraudulent level is very less for the people who have completed their high scool education. *
"""

# Comparing insured_occupation vs Fraud_reported.

plt.figure(figsize = (10 , 8))
sns.countplot(x = ins_data['insured_occupation'] , data = ins_data , hue = ins_data['fraud_reported'])
plt.xticks(rotation = 90)
plt.show()

"""*The people who are in the position exec-managerial have high fraud reports compared to others.*"""

# Comparing insured_hobbies and fraud_reported.

plt.figure(figsize = (10 , 8))
sns.countplot(x = ins_data['insured_hobbies'] , hue = ins_data['fraud_reported'] , data = ins_data)
plt.xticks(rotation = 90)
plt.show()

"""*The fraud report is high for the people who have the hobby of playing chess and cross fit.*"""

# Comparing insured_relationships and fraud_reported
sns.countplot(x = ins_data['insured_relationship'],data=ins_data,hue=ins_data['fraud_reported'],palette='Set2')
plt.xticks(rotation=90)
plt.show()

"""*The fraud report is high for the customers who have other relative and it is less for unmarried people.*"""

# Comparing incident_type and fraud_reported
sns.countplot(x = ins_data['incident_type'],data=ins_data,hue=ins_data['fraud_reported'],palette='Set2')
plt.xticks(rotation=45)
plt.show()

"""*The fraud reported when the type of incident is Multivehicle collision and single vehicle collision is high compared to Vehicle theft and parked vehicles.*"""

# Comparing collision_type and fraud_reported
sns.countplot(x = ins_data['collision_type'],data=ins_data,hue=ins_data['fraud_reported'],palette='Set2')
plt.show()

"""*The fraud reported is high when the  collision type is Rear Collision. When the collision type is Side and front the fraud reported is similar.*"""

# Comparing incident_Severeity and fraud_reported
sns.countplot(x = ins_data['incident_severity'],data=ins_data, hue=ins_data['fraud_reported'] ,palette='Set2')
plt.show()

"""The fraud report is high when the type of damage is Major damage and fraud commited is the least when the type of damage Trivial Damage."""

# Comparing authorities_contacted and fraud_reported
sns.countplot(x = ins_data['authorities_contacted'],data=ins_data,hue=ins_data['fraud_reported'],palette='Set1')
plt.show()

"""*The police contacted cases are very high.*"""

# Comparing incident_state and fraud_reported
sns.catplot(x=ins_data['incident_state'],data=ins_data,kind='count',col=ins_data['fraud_reported'],palette='RdBu')
plt.show()

"""*Most fradulant cases have taken place in the state "SC" followed by "NY" and the least in the state "PA".*"""

# Comparig incident_city and fraud_reported
sns.catplot(x = ins_data['incident_city'],kind='count',data=ins_data,hue=ins_data['fraud_reported'],palette="bright")
plt.xticks(rotation=90)
plt.show()

"""*The cities Riverwood and Northbrook have very less fraud reports compared to others.*"""

# Comparing property_damage and fraud_reported
sns.catplot(x = ins_data['property_damage'],kind='count',data=ins_data,hue=ins_data['fraud_reported'],palette="crest")
plt.show()

"""*The number of fraudulant cases reported when property damage is not there is higher.*"""

# Comparing police_report_available and fraud_reported
sns.catplot(x = ins_data['police_report_available'],kind='count',data=ins_data,hue=ins_data['fraud_reported'],palette="bright")
plt.show()

"""*The number of fraudulant cases are more when there are no police reports available.*"""

fig,axes=plt.subplots(2,2,figsize=(12,10))

# Comparing insured_sex and age
sns.violinplot(x=ins_data['insured_sex'],y=ins_data['age'],ax=axes[0,0],data=ins_data,hue=ins_data["fraud_reported"],split=True)

# Comparing policy_state and witnesses
sns.violinplot(x=ins_data['policy_state'],y=ins_data['witnesses'],ax=axes[0,1],data=ins_data,hue=ins_data["fraud_reported"],split=True)

# Comparing csl_per_accident and property_claim
sns.violinplot(x=ins_data['csl_per_accident'],y=ins_data['property_claim'],ax=axes[1,0],data=ins_data,hue=ins_data["fraud_reported"],split=True)

# Comparing csl_per_person and age
sns.violinplot(x=ins_data['csl_per_person'],y=ins_data['age'],ax=axes[1,1],data=ins_data,hue=ins_data["fraud_reported"],split=True)
plt.show()

"""The fraud report is high for both the males-females having age between 30-45.\
The people who own the policy state "IN" have high fraud report.\
The person who has csl per accidemt insurance by claimimg property in the range 5000-15000 have the fraud report.\
The csl_per_person with age 30-45 are facing the fraudulent reports.

# **Multi-Variate Analysis**
"""

sns.pairplot(ins_data,hue="fraud_reported")
plt.show()

"""# *Checking for Outliers*"""

for i in numerical_col :
  sns.boxplot(ins_data[i])
  plt.show()

"""*The features:age, policy_annaul_premium, total_claim_amount, property_claim, incident_month have outliers.*"""

features = ins_data[['age', 'policy_annual_premium', 'total_claim_amount', 'property_claim' , 'incident_month']]
# from scipy.stats import zscore
# z = np.abs(zscore(features))
# z

#features = ins_data[['age', 'policy_annual_premium', 'total_claim_amount', 'property_claim']]
# Convert all columns in features to numeric
features = features.apply(pd.to_numeric, errors='coerce')
from scipy.stats import zscore
z = np.abs(zscore(features))
z

new_data = ins_data[(z<3).all(axis=1)]
new_data.head()

print(ins_data.shape)
print(new_data.shape)

data_loss = (1000-996)/1000*100
data_loss

"""*We have lost 0.4%(4rows) of data after outliers remving.*

# *Checking for Skewness *
"""

new_data.drop(['policy_state'] , axis = 1)

new_data.columns

new_data.drop(['insured_occupation'] , axis = 1 , inplace = True)

new_data.drop(['policy_state' , 'insured_sex', 'insured_education_level','incident_type', 'collision_type',
       'incident_severity', 'authorities_contacted', 'incident_state',
       'incident_city','property_damage','police_report_available','auto_make',
       'auto_model', 'fraud_reported','incident_day'], axis = 1 , inplace = True)

new_data.drop(['insured_hobbies'] , axis = 1 , inplace = True)

new_data.drop(['insured_relationship'] , axis = True , inplace = True)

new_data.skew()

"""The following features contains skewness:

1. total_claim_amount
2. vehicle_claim
3. incident_month
4. csl_per_accident

*# Removing Skewness using yeo-johnson method*
"""

skew = ["total_claim_amount","vehicle_claim","incident_month","csl_per_accident"]

from sklearn.preprocessing import PowerTransformer
scaler = PowerTransformer(method='yeo-johnson')

new_data[skew] = scaler.fit_transform(new_data[skew].values)
new_data[skew].head()

new_data.skew()

"""*The Skewness removed from dataset successfully.*"""

for i in new_data[skew] :
  sns.distplot(new_data[i])
  plt.show()

"""*The data looks almost normally distributed.*

# *Encoding the categorical Data *
"""

new_data.info()

from sklearn.preprocessing import LabelEncoder

Le = LabelEncoder()
# iterate through each column in the cat_col list
for col in cat_col:
  # check if the column is of object type (string)
  if ins_data[col].dtype == 'object':
    # if it is, apply the LabelEncoder to that column
    # Convert the column to strings before applying LabelEncoder
    ins_data[col] = ins_data[col].astype(str)
    ins_data[col] = Le.fit_transform(ins_data[col])
ins_data[cat_col].head()

"""# Correlation"""

corr = new_data.corr()
corr

# Visualizing by Heatmap

plt.figure(figsize = (35,35))
sns.heatmap(corr , annot = True , fmt = '0.2f')
plt.show()

"""Observations from correlation:
1. There is very less correlation between the target and the label.
2. We can observe the most of the columns are highly correlated with each other which leads to the multicollinearity problem.
3. We will check the VIF value to overcome with this multicollinearity problem.
"""

x = ins_data.drop(['fraud_reported'] , axis = 1)
y = ins_data['fraud_reported']

"""# Standard Scaler"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x = pd.DataFrame(scaler.fit_transform(x) , columns = x.columns)
x.head()

"""# Checking for Multicolinearity"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif["VIF values"] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]
vif["Features"] = x.columns
vif

"""*Drop total_claim_amount as VIF>10.*"""

x.drop(['total_claim_amount'] , axis = 1 , inplace = True)

vif = pd.DataFrame()
vif["VIF values"] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]
vif["Features"] = x.columns
vif

"""*Drop csl_per_person as VIF>10.*"""

x.drop(['csl_per_person'] , axis = 1 , inplace = True)

vif = pd.DataFrame()
vif["VIF values"] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]
vif["Features"] = x.columns
vif

"""*We have successfully removed Multicolinearity from our dataset.*"""

y.value_counts()

"""# Oversampling"""

from imblearn.over_sampling import SMOTE
SM = SMOTE()
x , y = SM.fit_resample(x , y)

y.value_counts()

"""# **MODEL BUILDING**

*Searching for the best Random State.*
"""

from sklearn.model_selection import train_test_split as TTS
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

maxAccu = 0
maxRS = 0
for i in range(1,200) :
  x_train , x_test , y_train , y_test = TTS(x , y , test_size = 0.30 , random_state = i)
  RFC = RandomForestClassifier()
  RFC.fit(x_train , y_train)
  pred = RFC.predict(x_test)
  acc = accuracy_score(y_test , pred)
  if acc>maxAccu :
    maxAccu = acc
    maxRS = i
print("Best Accuracy Score is " , maxAccu , "at random state" , maxRS)

x_train,x_test,y_train,y_test=TTS(x,y,test_size=.30,random_state=maxRS)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from xgboost import XGBClassifier as xgb
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, accuracy_score
from sklearn.model_selection import cross_val_score

"""# ***Random Forest Classifier***"""

RFC = RandomForestClassifier()
RFC.fit(x_train,y_train)
predRFC = RFC.predict(x_test)
print(accuracy_score(y_test,predRFC))
print(confusion_matrix(y_test,predRFC))
print(classification_report(y_test,predRFC))

cm = confusion_matrix(y_test,predRFC)

x_axis_labels = ["NO","YES"]
y_axis_labels = ["NO","YES"]

sns.heatmap(cm, annot = True, fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for Random Forest Classifier')
plt.show()

"""# ***Decision Tree Classifier***"""

DTC = DecisionTreeClassifier()
DTC.fit(x_train , y_train)
predDTC = DTC.predict(x_test)
print(accuracy_score(y_test , predDTC))
print(confusion_matrix(y_test , predDTC))
print(classification_report(y_test,predDTC))

cm = confusion_matrix(y_test,predDTC)

x_axis_labels = ["NO","YES"]
y_axis_labels = ["NO","YES"]

sns.heatmap(cm, annot = True,fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for Decision Tree Classifier')
plt.show()

"""# ***Support Vector Machine Classifier***"""

svc = SVC()
svc.fit(x_train,y_train)
predSVC = svc.predict(x_test)
print(accuracy_score(y_test , predSVC))
print(confusion_matrix(y_test , predSVC))
print(classification_report(y_test,predSVC))

cm = confusion_matrix(y_test , predSVC)

x_axis_labels = ["NO" , "YES"]
y_axis_labels = ["NO" , "YES"]


sns.heatmap(cm, annot = True,fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for Support Vector Machine Classifier')
plt.show()

"""# ***Gradient Booster Classifier***"""

GB = GradientBoostingClassifier()
GB.fit(x_train,y_train)
predGB = GB.predict(x_test)
print(accuracy_score(y_test,predGB))
print(confusion_matrix(y_test,predGB))
print(classification_report(y_test,predGB))

cm = confusion_matrix(y_test , predGB)

x_axis_labels = ["NO" , "YES"]
y_axis_labels = ["NO" , "YES"]


sns.heatmap(cm, annot = True,fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for Gradient Booster Classifier')
plt.show()

"""# ***AdaBoost Classifier***"""

ABC = AdaBoostClassifier()
ABC.fit(x_train,y_train)
predABC = ABC.predict(x_test)
print(accuracy_score(y_test,predABC))
print(confusion_matrix(y_test,predABC))
print(classification_report(y_test,predABC))

cm = confusion_matrix(y_test,predABC)

x_axis_labels = ["NO","YES"]
y_axis_labels = ["NO","YES"]

sns.heatmap(cm, annot = True, fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for AdaBoost Classifier')
plt.show()

"""# ***Bagging Classifier***"""

BC = BaggingClassifier()
BC.fit(x_train,y_train)
predBC = BC.predict(x_test)
print(accuracy_score(y_test,predBC))
print(confusion_matrix(y_test,predBC))
print(classification_report(y_test,predBC))

cm = confusion_matrix(y_test,predBC)

x_axis_labels = ["NO","YES"]
y_axis_labels = ["NO","YES"]

sns.heatmap(cm, annot = True, fmt = ".0f", xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for Bagging Classifier')
plt.show()

"""# ***Extra Tree Classifier***"""

ET = ExtraTreesClassifier()
ET.fit(x_train,y_train)
predET = ET.predict(x_test)
print(accuracy_score(y_test,predET))
print(confusion_matrix(y_test,predET))
print(classification_report(y_test,predET))

cm = confusion_matrix(y_test,predET)

x_axis_labels = ["NO","YES"]
y_axis_labels = ["NO","YES"]

sns.heatmap(cm, annot = True, fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for ExtraTrees Classifier')
plt.show()

"""# ***XG BOOST CLASSIFIER***"""

XGB = xgb(verbosity=0)
XGB.fit(x_train,y_train)
predXGB = XGB.predict(x_test)
print(accuracy_score(y_test,predXGB))
print(confusion_matrix(y_test,predXGB))
print(classification_report(y_test,predXGB))

cm = confusion_matrix(y_test,predXGB)

x_axis_labels = ["NO","YES"]
y_axis_labels = ["NO","YES"]

sns.heatmap(cm, annot = True, fmt = ".0f",xticklabels=x_axis_labels,yticklabels=y_axis_labels)

plt.xlabel("PREDICTED LABEL")
plt.ylabel("TRUE LABEL")
plt.title('Confusion Matrix for XG Boost Classifier')
plt.show()

"""# ***Checking Cross Validation Score***"""

from sklearn.model_selection import cross_val_score

print("Cross_Validation_Score using Random Forest Classifier:",cross_val_score(RFC,x,y,cv=5).mean())

print("Cross_Validation_Score using Decision Tree Classifier:",cross_val_score(DTC,x,y,cv=5).mean())

print("Cross_Validation_Score using Support Vector Classifier:",cross_val_score(svc,x,y,cv=5).mean())

print("Cross_Validation_Score using Gradient Boosting Classifier:",cross_val_score(GB,x,y,cv=5).mean())

print("Cross_Validation_Score using AdaBoosting Classifier:",cross_val_score(ABC,x,y,cv=5).mean())

print("Cross_Validation_Score using Bagging Classifier:",cross_val_score(BC,x,y,cv=5).mean())

print("Cross_Validation_Score using ExtraTrees Classifier:",cross_val_score(ET,x,y,cv=5).mean())

print("Cross_Validation_Score using XG Boost Classifier:",cross_val_score(XGB,x,y,cv=5).mean())

"""As per the above observations :     

# *Best Model - ExtraTrees Classifiers*

# ***Hyperparameter Tuning***
"""

from sklearn.model_selection import GridSearchCV

parameters = {'criterion':['gini','entrophy'],
             'max_features':['auto','sqrt','log2'],
             'max_depth':[0,10,20],
             'n_jobs':[-2,-1,1],
             'n_estimators':[50,100,200,300]}

GCV = GridSearchCV(ExtraTreesClassifier(),parameters,cv=5)

GCV.fit(x_train,y_train)

GCV.best_params_

Insurance_model =  ExtraTreesClassifier(criterion='gini',max_features='auto',max_depth=20,n_estimators=300,n_jobs=-2)
Insurance_model.fit(x_train,y_train)
pred = Insurance_model.predict(x_test)
acc = accuracy_score(y_test,pred)
print(acc*100)

"""# *We get accuracy of 92.47% after hyperparameter tuning.*

# Plotting ROC and compare AUC for all the models used
"""

import matplotlib.pyplot as plt
from sklearn import datasets, model_selection, metrics
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import RocCurveDisplay

# Load dataset and split into training and testing sets
data = datasets.load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=15)

# Initialize models
ET = ExtraTreesClassifier(random_state=15)
RFC = RandomForestClassifier(random_state=15)
svc = SVC(kernel='linear', probability=True, random_state=15)
GB = GradientBoostingClassifier(random_state=15)
ABC = AdaBoostClassifier(random_state=15)
BC = BaggingClassifier(random_state=15)
XGB = XGBClassifier(random_state=15)

# Fit models
ET.fit(X_train, y_train)
RFC.fit(X_train, y_train)
svc.fit(X_train, y_train)
GB.fit(X_train, y_train)
ABC.fit(X_train, y_train)
BC.fit(X_train, y_train)
XGB.fit(X_train, y_train)

# Plot ROC curves using RocCurveDisplay
fig, ax = plt.subplots(figsize=(10, 10))
RocCurveDisplay.from_estimator(ET, X_test, y_test, ax=ax, name='Extra Trees')
RocCurveDisplay.from_estimator(RFC, X_test, y_test, ax=ax, name='Random Forest')
RocCurveDisplay.from_estimator(svc, X_test, y_test, ax=ax, name='SVC')
RocCurveDisplay.from_estimator(GB, X_test, y_test, ax=ax, name='Gradient Boosting')
RocCurveDisplay.from_estimator(ABC, X_test, y_test, ax=ax, name='AdaBoost')
RocCurveDisplay.from_estimator(BC, X_test, y_test, ax=ax, name='Bagging')
RocCurveDisplay.from_estimator(XGB, X_test, y_test, ax=ax, name='XGBoost')

# Customize and show plot
plt.legend(prop={'size':11}, loc='lower right')
plt.title('AUC ROC Curves')
plt.show()

"""# Plotting ROC for the Best Model"""

# Load dataset and split into training and testing sets
data = datasets.load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=15)

# Initialize models
ET = ExtraTreesClassifier(random_state=15)
RFC = RandomForestClassifier(random_state=15)
svc = SVC(kernel='linear', probability=True, random_state=15)
GB = GradientBoostingClassifier(random_state=15)
ABC = AdaBoostClassifier(random_state=15)
BC = BaggingClassifier(random_state=15)
XGB = XGBClassifier(random_state=15)

# Fit models
ET.fit(X_train, y_train)
RFC.fit(X_train, y_train)
svc.fit(X_train, y_train)
GB.fit(X_train, y_train)
ABC.fit(X_train, y_train)
BC.fit(X_train, y_train)
XGB.fit(X_train, y_train)

# Plot ROC curves using RocCurveDisplay
fig, ax = plt.subplots(figsize=(10, 10))
RocCurveDisplay.from_estimator(ET, X_test, y_test, ax=ax, name='Extra Trees')

"""# Saving the Model"""

import pickle
filename = 'insurance.pkl'
pickle.dump(RFC, open(filename, 'wb'))

prediction = Insurance_model.predict(x_test)
prediction

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Ensure that the split is correct with random_state=15
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)

# Train a model (using ExtraTreesClassifier as an example) with random_state=15
Insurance_model = ExtraTreesClassifier(random_state=15)
Insurance_model.fit(X_train, y_train)

# Make predictions
predictions = Insurance_model.predict(X_test)

# Ensure y_test is converted to a numpy array
a = np.array(y_test)

# Check lengths to ensure they match
print(f"Length of predictions: {len(predictions)}")
print(f"Length of y_test: {len(a)}")

# Create DataFrame
df = pd.DataFrame()
df["Predicted"] = predictions
df["Original"] = a

# Display DataFrame
print(df)

